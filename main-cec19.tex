\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% addtional packages
\usepackage{algorithm,algorithmic}
\usepackage{amsthm,amsmath,amssymb}

% additoonal envs.
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{note}{Note}[section]
\newtheorem{example}{Example}[section]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\title{A Metaheuristic for Bilevel Optimization Using Tykhonov Regularization and the Quasi-Newton Method}

\author{\IEEEauthorblockN{Jes\'us-Adolfo Mej\'ia-de-Dios}
\IEEEauthorblockA{\textit{Artificial Intelligence Research Center} \\
\textit{University of Veracruz}\\
Xalapa, Veracruz, Mexico \\
jesusmejded@gmail.com}
\and
\IEEEauthorblockN{Efr\'en Mezura-Montes}
\IEEEauthorblockA{\textit{Artificial Intelligence Research Center} \\
\textit{University of Veracruz}\\
Xalapa, Veracruz, Mexico \\
emezura@uv.mx}
}

\maketitle

\begin{abstract}
This work presents a population-based metaheuristic approach using Tykhonov
regularization and a quasi-Newton method, called Quasi-Newton Bilevel Centers
Algorithm (QBCA), to deal with bilevel optimization problems. Tykhonov regularization
for bilevel optimization is adopted to handle problems with nonunique lower level
solutions. Besides, a quasi-Newton method is adapted to deal with infeasible solutions
in the lower level. The performance of this proposal is assessed by using representative
test functions for bilevel optimization. The results based on accuracy and number
of evaluations are promising when QBCA is compared against the efficient algorithm
BLEAQ-II.
\end{abstract}

\begin{IEEEkeywords}
bilevel optimization, evolutionary algorithm, quasi-Newton, Tykhonov regularization
\end{IEEEkeywords}


\section{Introduction} % (fold)
\label{sec:introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%--------------------------

The first concepts on Bilevel Optimization (BO) were introduced in 1934 by Von
Stackelberg \cite{von2010market}. Nowadays, BO is known as a new kind of optimization
problem which is gaining interest by researchers and practitioners. A BO problem
contains a nested optimization problem as a constraint; such hierarchical structure
is illustrated in Fig. \ref{fig:bilevel}. Both optimization levels can be constrained,
unconstrained, single and/or multi-objective, discrete and/or continuous
\cite{bard2013practical,dempe2002foundations}. Many real-world problems keep a hierarchical
structure, then BO can be useful to model them, e.g. a decision-making process,
where a upper level authority (leader) optimizes their objectives restricted to
optimal decisions/solutions given by a  lower level authority (follower)
\cite{brotcorne2001bilevel,kalashnikov2010comparison,sinha2015transportation,von1945theory,wang2014bilevel}.

Due to the importance of BO, many authors have proposed different solutions
for those problems, from mathematical approaches (Karush-Kuhn-Tucker condition for
single-level reduction, gradient based algorithm, etc.) \cite{dempe2002foundations,shi2005extended}
to swarm intelligence (particle swarm optimization) and evolutionary algorithms
(genetic algorithms, evolution strategies, differential evolution)
\cite{derrac2011practical,angelo2013differential,li2006hierarchical}.

In order to handle BO problems three strategies can be identified: (1) single-level
reduction (2) nested and (3) penalty methods. Single-level reduction is restricted
to smooth enough functions and it is used to transform a BO problem into a single-level/traditional
problem by using Karush-Kuhn-Tucker conditions. As a consequence, such strategy
may be not easy to apply in some real-world problems  \cite{dempe2002foundations,colson2007overview}. %
Nested strategies can be effective when solve BO problems in a direct way. However, 
a high computational cost is required when high-dimensionality is present or when
objective functions are expensive to compute. Penalty methods are used to handle
constraints in a constrained problem so as to transform it into an unconstrained
optimization problem by adding penalty factors to control the degree of penalization.
This strategy is often hard to calibrate, particularly for large-scale problems.
Nevertheless, it is simple to understand and also easy to apply \cite{savard1994steepest,white1993penalty}.
Finally, it has been noted that the issue of non-unique optimal lower level
solutions has not been explored in this type of approaches.

As mentioned above, among population-based metaheuristics for BO problems, most
research efforts are focused on swarm intelligence and evolutionary algorithms.
Such preference is mainly because those methods have been successfully applied
to solve single-level optimization problems. However, their computational cost
in a BO problem, based on the number of evaluations, can be significantly high,
particularly in the lower level of the BO problem. In contrast, the combination
of population-based metaheuristics with mathematical programming methods may
provide, considering some assumptions, efficient and accurate approaches where
their computational cost can be reduced \cite{sinha2013efficient,wang2005evolutionary}.
Furthermore, to the best of the authors' knowledge, Tykhonov Regularization has
not been incorporated in this type of algorithms. 

This is the main motivation of this research work, which uses the Tykhonov
Regularization \cite{dempe2002foundations} to handle problems with non-unique
lower level solutions in the combination of a population-based algorithm, originally
designed to solve global optimization problems \cite{Mejia2018}, with a quasi-Newton
method \cite{fletcher2013practical}. Here, an unconstrained nested scheme is
considered, where both objective functions are nonnegative (not a restrictive
assumption) and should be maximized.

This paper is organized as follows: Section \ref{sec:qca} describes the proposed
approach, including explanations of the proposed population-based algorithm based
on an operator inspired in the center of mass, the quasi-Newton method to deal
with infeasible solutions in the lower level, and the Tykhonov Regularization to
deal with non-unique optimal solutions in the lower level. Section \ref{sec:numerical_result}
includes the experiments proposed, their results and corresponding discussion.
Finally, Section \ref{sec:conclusions} summarizes the conclusions and states
the future work.


\section{A Brief Description of Bilevel Optimization} % (fold)
\label{sec:a_brief_description_of_bilevel_optimization}

%--------------------------------------------------
%--------------------------------------------------
The definition of a BO problem can be given in terms of the traditional
optimization problem definition. Thus, we start by describing a traditional optimization
problem. Without loss of generality, an optimization problem can be defined as
finding the set \cite{chong2013introduction,rao2009engineering}:
% 
\begin{align}
    \label{eqn:Xargmin}
    X^* &= \arg \min_{\vec{x} \in X} f(\vec{x}) \\ \nonumber
    &= \{ \vec{x}^* \in X \ : \ f(\vec{x}^*) \leq f( \vec{x} ), \ 
    % 
    \forall
    % 
    \vec{x} \in X \},
\end{align}
% 
where a bounded below function $f$ , i.e., $f(\vec{x}^*)> -\infty$ is called objective
function. $X$ is a $n$-dimensional parameter space, i.e. $X \subset \mathbb{R}^n$
is the domain for $\vec{x}$ representing constraints on
allowable values for $\vec{x}$.\\

Now, we are able to define a general BO problem with
single-objective functions at both levels
\cite{bard2013practical,dempe2002foundations}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{img/bilevel.pdf}
    \caption{ Diagram of a bilevel optimization problem. In step 1, a vector of %
              upper level parameters $x$ is selected; the problem $\min_{y\in Y} %
              f(x, y)$ is solved in step 2; consequently in step 3, we can evaluate %
              the feasible solution $(x,\ y^*)$ at the upper level objective function. %
            }
    \label{fig:bilevel}
\end{figure}
% 

\begin{definition}\textbf{(Bilevel Optimization Problem)}
    The 5-tuple $(F, \ f, \ X, \ Y, \ \mathbb{R} )$ is called a bilevel optimization
    problem if the the upper-level/leader's objective function
    $F: X \times Y \to \mathbb{R}$ and the lower-level/follower's objective
    function $f: X \times Y \to \mathbb{R}$. Then, the optimization process is
    formulated as follows:
    % 
    % 
    \noindent
    Minimize
    \begin{equation}
        F(\vec{x},\ \vec{y}) \text{ with } \ \vec{x} \in X , \ \vec{y} \in Y 
        \label{eqn:minF1}
    \end{equation}
    % 
    subject to
    % 
    \begin{align}
        \label{eqn:y-arg}
        &\vec{y} \in \Psi(\vec{x}) = \arg \min_{z\in Y} \{ f(\vec{x}, \vec{z}) \ : g_j(\vec{x}, \vec{z})  \leq 0 \} \\
        &  G_{i}(\vec{x}, \vec{y}) \leq 0,
        % 
    \end{align}
    % 
    where $X \subseteq \mathbb{R}^n $
    and $Y \subseteq \mathbb{R}^m$.
    Here, $G_i$ (for $i = 1,2,\ldots,I$) and $g_j$ (for $j = 1,2,\ldots,J$) are
    inequality constraints for the upper and lower level, respectively.
\end{definition}
% 
Note that, for simplicity, equality constraints were not considered in the definition
above. Figure \ref{fig:bilevel} shows a schematic diagram of a BO problem.
% 

How complicated are bilevel optimization problems?  In 1985, Jeroslow probed that
linear bilevel optimization problems are NP-hard \cite{jeroslow1985polynomial}.
After that, Hansen et al. showed that some BO problems are strongly NP-hard, since
evaluating a solution in a bilinear programming problem is also NP-hard
\cite{hansen1992new,vicente1994descent}. On the other hand, many real-world problems can
be naturally modeled as BO problems \cite{sinha2018review} where objective functions
are non-linear, e.g., taxation, border security problems, transportation problems,
machine learning algorithms tuning, among others \cite{bard2013practical,sinha2018review,arroyo2010bilevel}.
Therefore, BO problems require effective algorithms to approximate competitive
solutions.

% section a_brief_description_of_bilevel_optimization (end)


\section{Quasi-Newton Bilevel Centers Algorithm (QBCA)} % (fold)
\label{sec:qca}

As a first step in the explanation of the proposed approach we define a
Two-population for Bilevel Optimization (BO).

\begin{definition}
    \label{def:pop2}
    Consider the unconstrained BO problem $(F, \ f, \ X, \ Y, \ \mathbb{R} )$. A
    Two-population with size $N$ is defined as follows:
    % 
    $$
        P = \{  (\vec{x}_i, \ \vec{y}_i) \in X \times Y \ : \
                \vec{x}_i \in X, \ \vec{y}_i \in Y, \ i=1,\ldots,N
            \}.
    $$     
\end{definition}
 
In Definition \ref{def:pop2}, each $\vec{x}_i \in X$ in a Two-population has a
corresponding $\vec{y}_i \in Y$ which is an approximation of a feasible solution,
i.e., $(\vec{x}_i, \ \vec{y}_j)$ for $i \neq j$ is  not a valid solution.

\subsection{Tykhonov regularization}
Regularization for a BO problem is used to handle problems with nonunique lower
level solutions.  Tykhonov regularization is applied when the lower level problem
is convex and has not a unique optimal solution but the upper level objective function
is strongly convex with respect to $\vec{x}$ (see \cite{dempe2002foundations}).
Based on this, we can define two penalization functions.

\begin{definition}
    Suppose $(F, \ f, \ X, \ Y, \ \mathbb{R} )$ is a bilevel optimization problem,
    then Eq. \ref{eqn:regularization} and Eq \ref{eqn:regularization2}:
    \begin{align}
        \label{eqn:regularization}
        \varphi_f (\vec{x},\ \vec{y}) := \alpha F(\vec{x},\ \vec{y}) +  f(\vec{x},\ \vec{y}) \\
        \label{eqn:regularization2}
        \varphi_F (\vec{x},\ \vec{y}) := F(\vec{x},\ \vec{y}) +  \beta f(\vec{x},\ \vec{y})
    \end{align}
    are two penalization functions, where $\alpha, \beta \in (0,\ 1]$.
\end{definition}
% 
Eq. \ref{eqn:regularization} is precisely known as Tykhonov regularization, which is a
perturbation of a bilevel optimization problem where the lower level problem has
not a unique optimal solution but the upper level objective function $F$ is 
strongly convex, then the problem in Eq. \ref{eqn:relaxed}:
\begin{equation}
    \min_{y \in Y} \{ f(\vec{x},\vec{y}) + \alpha F(\vec{x}, \vec{y}) : g_j(\vec{x}, \vec{y}), j=1,2,\ldots, J\} 
    \label{eqn:relaxed}
\end{equation}
% 
has a uniquely determined optimal solution for each $\alpha > 0$. 
Moreover, Tykhonov regularization provides theorems for convergence to the optimal
solution when some assumptions are satisfied \cite{dempe2002foundations}. 
%\\

Here, Eq. \ref{eqn:regularization2} will be used to give a bias to upper level
regions where the leader's objective function is optimized with a small perturbation
of the follower's objective function values when the nonnegative parameter $\beta$
is small enough. 



\subsection{The center of mass operator} % (fold)
\label{sub:the_center_operator}
% 

A variation operator inspired in the center of mass and presented in \cite{Mejia2018}
is used for the population-based search at both levels of the BO problem. In order
to compute the center of mass in a minimization problem, we suggest the following
strategy:
% 
\[
    m_F(\vec{x}, \vec{y}) = \left[ \max_{(\vec{x}, \vec{y})\in U} |\varphi_F (\vec{x}, \vec{y}; \beta)| \right]
                            - \varphi_F (\vec{x}, \vec{y}; \beta).
\]

If both levels there are maximization problems with non-negative objective functions
(ECA assumptions \cite{Mejia2018}), then $m_F(\vec{x}, \vec{y}) = \varphi_F (\vec{x}, \vec{y}; \beta)$.
Hence, such operator is defined in Eq. \ref{eqn:centerX}:
% 
\begin{align}
    \label{eqn:centerX}
    \vec{c}_X &= \dfrac{1}{W} \sum_{(\vec{x}, \vec{y})\in U} m_F(\vec{x},\ \vec{y}) \cdot \vec{x}\\
            W &= \sum_{(\vec{x}, \vec{y})\in U} m_F(\vec{x},\ \vec{y}), \nonumber
\end{align}
% 
where $U \subset P$ is generated by choosing its elements uniformly at random.
Note that, at the upper level, $\varphi_F (\vec{x},\ \vec{y})$ is used to penalize
the $f$ values when $\beta\in (0, 1]$. At the lower level, $\varphi_f (\vec{x},\ \vec{y})$
is used to handle nonunique lower level solutions (when Tykhonov assumptions are 
satisfied) and penalizes the $F$ values when $\alpha$ is small enough. Thus, $c_X$
is biased toward smaller values of $F$ rather than small values of $f$. According
to that behavior, a new vector of parameters for the upper level is generated as
in Eq. \ref{eqn:leaderp}:
% 
\begin{equation}
    \vec{p} = \vec{x} + \eta_{X} (\vec{c}_X - \vec{u}_{\text{worst}}),
    \label{eqn:leaderp}
\end{equation}
% 
where $\eta_{X} \in (0, \eta_{\max}]$ and $\eta_{\max}$ is the user-defined stepsize and  
$
    (\vec{u}_{\text{worst}}, \vec{y}) \in \arg \max \{\varphi_F(\vec{x}, \vec{y} )  \ : \ (\vec{x}, \vec{y}) \in U \} 
$ %
% 
and this new direction $\vec{p}$ will be called \textit{current-to-center} direction. %
On the the other hand, the center of mass for the lower level is defined on
$V \subset P$ in Eq. \ref{eqn:centerY}:
% 
\begin{align}
    \label{eqn:centerY}
    \vec{c}_Y &= \sum_{(\vec{x}, \vec{y})\in V} m_f(\vec{x},\ \vec{y}) \cdot \vec{y},\\
         W &= \sum_{(\vec{x}, \vec{y})\in V} m_f(\vec{x},\ \vec{y}) , \nonumber
\end{align}
% 
where $m_f(\vec{x}, \vec{y}) = \varphi_f(\vec{x}, \vec{y}; \alpha )$ if ECA assumptions
are satisfied, otherwise the mass is computed as follows:
% 
\[
    m_f(\vec{x}, \vec{y}) =
        \left[ \max_{(\vec{x}, \vec{y})\in U} |\varphi_f (\vec{x}, \vec{y}; \alpha)| \right]
        - \varphi_f (\vec{x}, \vec{y}; \alpha)
\]
% 
It can be shown that $\vec{c}_Y$ is biased toward smaller values of the relaxed
problem stated in Eq. \ref{eqn:relaxed} for small positive values of $\alpha$.
% 
The new direction $\vec{y}_c$ should be close to a feasible solution and biased
to the center of mass. Thus, it is necessary to ensure that letting
$(\vec{x}_\text{nearest}, \vec{y}_{\text{nearest}}) $ be in $P$ such that $\vec{x}$
is close enough to $\vec{p}$ in terms of euclidean norm, i.e.,
$ \| \vec{x}_{\text{nearest}} - \vec{p} \| = \min\{ \| \vec{x} - \vec{p} \| \ : \ (\vec{x}, \vec{y}) \in P  \} $.
Therefore,
% 
\begin{equation}
    \vec{y}_c = \vec{y}_{\text{nearest}} + \eta_{Y} \dfrac{\vec{c}_Y - \vec{v}_{\text{worst}}}{\| \vec{c}_Y - \vec{v}_{\text{worst}} \|},
    \label{eqn:yc}
\end{equation} %
% 
where $\eta_Y$ takes small positive values and 
% 
$
    (\vec{x}, \vec{v}_{\text{worst}}) \in \arg \max \{\varphi_f(\vec{x}, \vec{y} )  \ : \ (\vec{x}, \vec{y}) \in V \}
$. Note that Eq. \ref{eqn:yc} generates follower's parameters close to a feasible
solution when $\eta_{Y}$ is small enough and $f$ is stable (Lipschitz continuous)
around $\vec{y} \in \Psi(\vec{x}) $ for  $\vec{x} \in X$ (see \cite{dempe2002foundations}). %\\
% 

% 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{img/x-and-y.pdf}
    \caption{QBCA procedure scheme.}
    \label{fig:qca}
\end{figure}
% 
The strategy detailed above can be useful to approximate solution for a BO problem
when each solution in  $P$ is feasible and $\eta_{Y}$ is small enough. However,
the new solution $(\vec{p},\ \vec{y}_c)$ could be infeasible. The next section
is focused on moving $(\vec{p},\ \vec{y}_c)$ close to the feasible region
(see Fig. \ref{fig:qca}).


\subsection{Quasi-Newton method} % (fold)
\label{sub:nonsmooth_optimization_via_bfgs}

Quasi-Newton methods are known to be generalizations of the secant method for
finding the gradient's root of optimization problems in continuous search spaces \cite{fletcher2013practical,liu1989limited}.
In $D$-dimensional problems, the secant equation does not specify a unique solution,
and quasi-Newton methods differ in how they handle that issue. Quasi-Newton methods
are widely used for numerical optimization when the objective function is smooth
\textit{almost everywhere} \cite{lewis2013nonsmooth}, since the Hessian matrix
(second derivate of gradient) is approximated using updates specified by gradient
evaluations (in practice, gradient approximation). In theory, when $f$ is not
smooth in a point the algorithm should be stopped. 

\begin{algorithm}[htbp]
    \caption{BFGS-LL: Quasi-Newton method for the lower level problem.}
    \label{alg:BFGS-LL}
    \begin{algorithmic}[1]
        \STATE Compute $\vec{y}_c$ with $f$ differentiable at $(\vec{p},\ \vec{y}_c)$
        \STATE Set $H_0$ to a positive definite matrix (identity matrix $I$)
        \STATE Put $\vec{q}_0 \gets \vec{y}_c$ and $k \gets 0$
        \WHILE{the end criterion is not achieved}
            \STATE $\vec{h}_k \gets -H_k\nabla f(\vec{p}, \vec{q}_k)$
            \STATE Compute $\gamma_{k}\gets\arg \min f(\vec{p}, \ \vec{q} _{k}+ \gamma_k \vec{h} _{k})$.
            \STATE $\vec{q}_{k+1} \gets \vec{q}_k  + \gamma_k \vec{h}_k$
            \IF{ $f$ is not differentiable at $(\vec{p}, \ \vec{q}_{k+1})$}
                \STATE Stop
            \ENDIF
            \STATE $z_k \gets \nabla f(\vec{p}, \ \vec{q}_{k+1}) - \nabla f(\vec{p}, \ \vec{q}_{k}) $
            \STATE $\displaystyle A_k \gets I - (\vec{h}_k^T \vec{z}_k)^{-1} \vec{h}_k \vec{z}_k^T$
            \STATE $H_{k+1} \gets A_k H_k A_k^T + \gamma_k(\vec{h}_k^T\vec{z}_k)^{-1}\vec{h}_k \vec{h}_k^T$
                   which is a positive definite matrix and satisfies the secant
                   condition $H_{k+1} z_k = \gamma_k \vec{h}_k$
            \STATE $k \gets k + 1$
        \ENDWHILE
        \STATE \textbf{Return} $\vec{q}_k$
    \end{algorithmic}
\end{algorithm}

We adapted a popular quasi-Newton method known as Broyden-Fletcher-Goldfarb-Shanno
algorithm (BFGS) \cite{fletcher2013practical} in order to handle infeasible
solutions $(\vec{p}, \vec{y}_c)$ for bilevel optimization. Such adaptation is
described in Algorithm \ref{alg:BFGS-LL}.
% 

\subsection{Initial Population} % (fold)
\label{sub:initial_population}

% subsection initial_population (end)

The QBCA method should start with a feasible initial population, then the ECA
algorithm \cite{Mejia2018} is used to approximate feasible solution with a limited
number of evaluations at the lower level. The initial population is constructed
as follows:

\begin{enumerate}
    \item Set $P = \emptyset $.
    \item Generate $N$ vector of parameters $\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_N$
          uniformly at random at upper level.
    \item For each $i=1,\ 2,\ \ldots,N$ compute
            $\vec{y}_i \in \arg \min \{ f(\vec{x}_i, \vec{z}) : \vec{z} \in Y \}$
          by using ECA to approximate a global solution.
    \item Put $\vec{y}_c = \vec{y}_i$ and $\vec{p} = \vec{x}_i$ and apply the
          Algorithm \ref{alg:BFGS-LL} to improve the solution accuracy.
    \item Insert the improved solution $(\vec{x}_i, \vec{y}_i)$ in $P$.
\end{enumerate}

\subsection{Comparing Solutions} % (fold)
\label{sub:comparing_solutions}

Two solutions are compared in terms of the Tykhonov regularization $\varphi_f$
to prefer feasible solutions instead of minimum $F$ values in the relaxed problem
in Eq. \ref{eqn:relaxed}. That is derived from Theorem 7.16 in \cite{dempe2002foundations}
where if its hypotheses are satisfied the following fact is obtained:
% 
\[
    \lim_{\alpha \to 0+} \vec{x}_\alpha = \arg \min_{\vec{y} \in \Psi(\vec{x})} F(\vec{x}, \vec{y}),
\]
%
where $\vec{x}_\alpha = \arg \min\{\varphi_f (\vec{x},\ \vec{y}; \alpha) : \vec{y} \in Y\} $.
Moreover, for each $\alpha > 0$, the following result is obtained:
% 
\begin{align}
    \label{eqn:Fbound}
    F(\vec{x}_\alpha, \vec{y}) &\leq \min_{ \vec{x} \in X} \{ F(\vec{x}, \vec{y}) : \vec{y} \in \Psi(\vec{x}) \} \\
    \label{eqn:fbound}
    \min_{\vec{y} \in Y } f(\vec{x}, \vec{y}) &\leq f(\vec{x}_\alpha, \vec{y}^*).
\end{align}
% 
That provides some interesting facts when solutions are compared using $F$ or $f$,
i.e.,  equation \ref{eqn:Fbound} means that if we compare solutions only using the
function $F$ our proposal can prefer best values of $F$ instead feasible solutions.
On the other hand, Eq. \ref{eqn:fbound} means that feasible solutions will be
preferred when two solutions are compared only in  terms of $f$. Thus, it is
necessary compare solutions using both objective functions, and we use the Tykhonov
regularization $\varphi_f$ when the optimal values are unknown.



%%%%%%%%%%%%%%%%%%%%%%%%

% subsection evaluating_solutions (end)

\subsection{Complete algorithm}

Now, we can summarize our proposal QBCA to approximate a solution for a given bilevel
optimization problem: First, a Two-population $P \in X \times Y$ of size $N$ is
generated at random with uniform distribution, and for each solution $(\vec{x}, \vec{y})$
in $P$ where $\vec{y} \in \arg \min_{\vec{x}\in Y} \{ f(\vec{x},\vec{z})\}$
is computed by applying the algorithm described in Sections \ref{sub:initial_population},
inspired in\cite{Mejia2018}. 

After that, the $k$ elements in $U, V \in X\times Y$ are generated at random with
uniform distribution. With both sets $U$ and $V$ available, $\vec{p}$ and $\vec{y}_c$
are generated by using Eq. \ref{eqn:leaderp} and Eq. \ref{eqn:yc}, respectively.
The next step applies Algorithm \ref{alg:BFGS-LL} to $\vec{y}_c$ to approximate
a feasible solution $(\vec{p}, \vec{q})$. 

The new solution is then evaluated in terms of $\varphi_f$ (to prefer feasible
solutions instead of minimum $F$ values in the relaxed problem in Eq. \ref{eqn:relaxed}),
then the worst solution in $P$ is replaced by $(\vec{p}, \vec{q})$ if this new
one is indeed better than its parent solution $(\vec{x}, \vec{y})$. The algorithm
stops either when the maximum number of evaluations for the upper level is reached
or the desired accuracy is met or a selection ratio $s$ is smaller than a constant
$s_{\min}$. Here, the selection ratio is computed as follows: $ s = n /N$ where
$n$ is the number of new solutions added to $P$. The complete approach is presented in 
Algorithm \ref{alg:QBCA}.

\begin{algorithm}[htbp]
    \caption{QBCA pseudocode}
    \label{alg:QBCA}
    \begin{algorithmic}[1]
        \STATE Choose $k$, $\eta_{\max}$.
        \STATE $N \gets k * D$
        \STATE Initialize a Two-population $P\subset X\times Y$ with $N$ elements
        \WHILE{the end criterion is not achieved}
            \FOR {each $(\vec{x},\; \vec{y})$ in $P$}
                \STATE Generate $U \subset P$ and $V \subset P$ with $|U| = |V| = k$
                \STATE Choose $\eta_{X}$ and $\eta_{Y}$
                \STATE Compute $\vec{c_X}$ using $U$ with Eq. (\ref{eqn:centerX})
                \STATE Calculate $\vec{c_Y}$ using $V$ with Eq. (\ref{eqn:centerY})
                \STATE $ \vec{p} \gets \vec{x} + \eta_{X} (\vec{c}_X - \vec{u}_{\text{worst}})$
                \STATE Compute $ \vec{y}_c $ by using Eq. \ref{eqn:yc}
                \STATE $ \vec{q} $ is calculated using $\vec{y}_c$ and the Algorithm \ref{alg:BFGS-LL}
                
                \IF{$ \varphi_f (\vec{p},\ \vec{q}) < \varphi_f (\vec{x},\ \vec{y})  $}
                    \STATE Replace worst element in $P$ with $(\vec{p},\ \vec{q})$.
                \ENDIF
            \ENDFOR
        \ENDWHILE
        \STATE Report best solution in $P$
        % \EndProcedure
    \end{algorithmic}
\end{algorithm}

\section{Numerical Results and Discussion} % (fold)
\label{sec:numerical_result}

QBCA was implemented in the Julia Programing Language \cite{bezanson2017julia}
and it was tested using both 5-dimensional ($D = D_{UL} = 2 $ $ D_{LL} = 3$) and
10-dimensional SMD problems ($D = D_{UL} = D_{LL} = 5$)
\cite{sinha2014test,sinha2013efficient}. The first eight SMD test problems were
proposed in order to offer controlled scalability of difficulties at both levels.
Also, the optimal solutions are given for each problem. The principal properties
of SMD test problems are identified and we enumerate some of the most important:

\begin{itemize}
    \item Multi-modality at upper level (SMD7 and SMD8).
    \item Multi-modality at lower level (SMD3-SMD5 and SMD8).
    \item Multiple global solutions (SMD6).
\end{itemize}

The parameters adopted after preliminary
experiments were $k = 3$ (fixed), $\eta_{X} \in (0, \eta_{\max}]$ and $\eta_{Y} \in (0, 1/\eta_{\max}]$ are randomly chosen uniformly at random.
$\alpha = \beta = 1 / 20$ and $s_{\min} = 1/100$. 

% 
The maximum number of functions evaluations (MFEs) was $1000D_{UL}=5,000$ for the
upper and unlimited lower level evaluations.
% 

The  algorithm stopped either
when the accuracy ($1\times 10^{-4}$) was obtained or the MFEs was reached.

% 
QBCA (Algorithm \ref{alg:QBCA}) was compared against BLEAQ-II, which is a state-of-the-art
evolutionary algorithm for BO problems \cite{sinha2018review,sinha2013efficient}.
BLEAQ-II solves the nested problem by using quadratic approximations of the
$\Psi$-mapping which computes lower level solutions based on a function of upper
level variables. Moreover, BLEAQ-II showed competitive results since it reduces
the NFEs at the lower level. Here, BLEAQ-II run with the parameters suggested by
the authors in \cite{sinha2018review,sinha2017bilevel} and the stop criterion was
maintained.  However, the desired accuracy was set at $1\times 10^{-4}$, as in
QBCA, to promote a fair comparison.

Three experiments were carried out:
% 
\begin{enumerate}
    \item The analysis of the QBCA statistical results based on accuracy in both,
          upper and lower levels.
    \item The analysis of the QBCA statistical results based on NFEs in both,
          upper and lower levels. 
    \item A comparison against BLEAQ-II considering both criteria, accuracy and
          NFEs, in both levels as well.
\end{enumerate}
% 


\subsection{Experiment 1}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{img/SMD_2_3.pdf}
    \caption{Convergence plot for median upper level function value obtained from 31 independent runs of QBCA on SMD1-SMD5 and SMD8 problems with $D_{ul} = 2 $ and $ D_{ll} = 3$. Log scale is used for visualization purposes.}
\end{figure*}

Tables \ref{tab:ul-accur} and \ref{tab:ll-accur} include the statistical results
obtained by QBCA regarding accuracy in the upper and lower level, respectively.
It can be noted that QBCA provides good accuracy values in most test problems
and it is also robust in such behavior, mainly based in the standard deviation
values reported. 

\begin{table}[htbp]
    \caption{Upper Level Accuracy obtained from 31 independent runs by QBCA.}
    \label{tab:ul-accur}
    \centering
    \begin{tabular}{ccccc}
        \hline
        & Best &  Median  &  Worst &  Std. \\ \hline
        SMD1 & 1.3247E-05 & 7.0279E-05 %& 2.1850E-04 
        & 4.7632E-03 & 8.4390E-04 \\ \hline 
        SMD2 & 6.7102E-06 & 6.5342E-05 %& 6.0071E-05 
        & 9.9377E-05 & 2.8114E-05 \\ \hline 
        SMD3 & 1.5002E-05 & 5.5641E-05 %& 5.7023E-05 
        & 9.9667E-05 & 2.4343E-05 \\ \hline 
        SMD4 & 7.1488E-06 & 5.0910E-05 %& 5.1723E-05 
        & 9.5625E-05 & 2.4660E-05 \\ \hline 
        SMD5 & 1.3112E-05 & 6.7266E-05 %& 6.1904E-02 
        & 1.9170E+00 & 3.4429E-01 \\ \hline 
        SMD6 & 1.2462E+01 & 1.2483E+01 %& 1.2480E+01 
        & 1.2492E+01 & 7.9161E-03 \\ \hline 
        SMD7 & 9.3416E-01 & 1.0252E+00 %& 1.8582E+00 
        & 1.7670E+01 & 3.0556E+00 \\ \hline 
        SMD8 & 3.5065E-05 & 7.8491E-05 %& 7.2548E-05 
        & 9.9948E-05 & 1.8556E-05 \\ \hline 
 
    \end{tabular}
\end{table}
% 

\begin{table}[htbp]
    \caption{Upper Level Accuracy obtained from 31 independent runs by QBCA. 5-dimensional}
    \label{tab:ul-accur-5}
    \centering
    \begin{tabular}{ccccc}
        \hline
             & Best &  Median  &  Worst &  Std. \\ \hline
        SMD1 & 1.7062E-06 & 3.7011E-05 & 9.9622E-05 & 2.5257E-05 \\ \hline 
        SMD2 & 2.3506E-06 & 3.8328E-05 & 9.5772E-05 & 3.1144E-05 \\ \hline 
        SMD3 & 1.5510E-08 & 2.6711E-05 & 9.5526E-05 & 2.7578E-05 \\ \hline 
        SMD4 & 1.4199E-06 & 3.7548E-05 & 9.4937E-05 & 3.3176E-05 \\ \hline 
        SMD5 & 2.2916E-06 & 3.7063E-05 & 9.9228E-05 & 2.9727E-05 \\ \hline 
        SMD6 & 1.6154E+00 & 1.0716E+01 & 1.2449E+01 & 2.3314E+00 \\ \hline 
        SMD7 & 7.7884E-01 & 7.7884E-01 & 2.9590E+00 & 3.9155E-01 \\ \hline 
        SMD8 & 2.3498E-06 & 5.8509E-05 & 9.7624E-05 & 2.4432E-05 \\ \hline 
 
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \caption{Lower Level Accuracy obtained from 31 independent runs by QBCA.}
    \label{tab:ll-accur}
    \centering
    \begin{tabular}{ccccc}
        \hline
        & Best &  Median  &  Worst &  Std. \\ \hline

        SMD1 & 1.4531E-06 & 1.2053E-05 %& 2.4444E-05 
        & 2.5890E-04 & 4.6808E-05 \\ \hline 
        SMD2 & 1.2672E-06 & 1.5472E-05 %& 2.1763E-05 
        & 8.5926E-05 & 2.1078E-05 \\ \hline 
        SMD3 & 2.1643E-06 & 1.3876E-05 %& 1.6668E-05 
        & 6.1028E-05 & 1.2625E-05 \\ \hline 
        SMD4 & 8.7042E-08 & 1.3845E-05 %& 2.0255E-05 
        & 8.4227E-05 & 2.1225E-05 \\ \hline 
        SMD5 & 1.1723E-06 & 1.1682E-05 %& 7.5928E-04 
        & 2.3115E-02 & 4.1491E-03 \\ \hline 
        SMD6 & 2.6988E-06 & 6.2809E-05 %& 9.2298E-05 
        & 4.0326E-04 & 8.6389E-05 \\ \hline 
        SMD7 & 2.2602E+02 & 3.7499E+02 %& 3.6682E+02 
        & 3.7500E+02 & 3.2090E+01 \\ \hline 
        SMD8 & 6.5062E-06 & 2.1320E-05 %& 2.3710E-05 
        & 4.6556E-05 & 1.2667E-05 \\ \hline 

    \end{tabular}
\end{table}

\begin{table}[htbp]
    \caption{Lower Level Accuracy obtained from 31 independent runs by QBCA. 5-dimensional}
    \label{tab:ll-accur-5}
    \centering
    \begin{tabular}{ccccc}
        \hline
             & Best &  Median  &  Worst &  Std. \\ \hline
        SMD1 & 4.5075E-11 & 6.5495E-06 & 6.2989E-05 & 1.7046E-05 \\ \hline 
        SMD2 & 3.1313E-08 & 5.9320E-06 & 6.1921E-05 & 1.5553E-05 \\ \hline 
        SMD3 & 1.0822E-09 & 4.7070E-06 & 5.9067E-05 & 1.4857E-05 \\ \hline 
        SMD4 & 9.4437E-09 & 2.3948E-06 & 5.0427E-05 & 1.4513E-05 \\ \hline 
        SMD5 & 1.9597E-09 & 8.2375E-06 & 7.3105E-05 & 2.0953E-05 \\ \hline 
        SMD6 & 9.3386E-09 & 4.1155E-05 & 3.4679E-02 & 7.1782E-03 \\ \hline 
        SMD7 & 1.2499E+02 & 1.2500E+02 & 1.2500E+02 & 9.8179E-04 \\ \hline 
        SMD8 & 1.6166E-07 & 7.5347E-06 & 1.7859E-05 & 5.8398E-06 \\ \hline 

    \end{tabular}
\end{table}

\subsection{Experiment 2}
Tables \ref{tab:ul-evals} and \ref{tab:ll-evals} present the statistic values
obtained by QBCA with respect to the NFEs computed in the upper and lower level,
respectively. As it was the case with respect to the accuracy results, the behavior
observed in the evaluations used in the upper level (Table \ref{tab:ul-evals})
was robust. In contrast, more variability was observed in the lower level
(Table \ref{tab:ll-evals}). 

\begin{table}[htbp]
    \caption{Number of upper level evaluations obtained from 31 independent runs by QBCA.}
    \label{tab:ul-evals}
    \centering
    \begin{tabular}{cccccc}
        \hline
        & Best &  Median &  Mean &  Worst &  Std. \\ \hline
        SMD1 & 1221 & 1553 & 1559 & 2110 & 193.7 \\ \hline 
        SMD2 & 1089 & 1525 & 1491 & 1755 & 148.3 \\ \hline 
        SMD3 & 1262 & 1580 & 1584 & 1886 & 149.8 \\ \hline 
        SMD4 & 876 & 1401 & 1382 & 2043 & 228.9 \\ \hline 
        SMD5 & 421 & 1615 & 1577 & 1844 & 244.6 \\ \hline 
        SMD6 & 1591 & 1981 & 1979 & 2461 & 172.3 \\ \hline 
        SMD7 & 421 & 5011 & 4717 & 5011 & 1135 \\ \hline 
        SMD8 & 2121 & 2432 & 2427 & 2836 & 186.8 \\ \hline 
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \caption{Number of upper level evaluations obtained from 31 independent runs by QBCA. 5-dimensional}
    \label{tab:ul-evals-5}
    \centering
    \begin{tabular}{cccccc}
        \hline
             & Best &  Median &  Mean &  Worst &  Std. \\ \hline
        SMD1 & 136 & 210 & 212 & 318 & 42 \\ \hline 
        SMD2 & 116 & 192 & 191 & 242 & 33 \\ \hline 
        SMD3 & 80 & 217 & 216 & 323 & 54 \\ \hline 
        SMD4 & 112 & 167 & 164 & 236 & 31 \\ \hline 
        SMD5 & 123 & 207 & 208 & 269 & 31 \\ \hline 
        SMD6 & 2000 & 2000 & 2000 & 2000 & 0 \\ \hline 
        SMD7 & 2000 & 2000 & 2000 & 2000 & 0 \\ \hline 
        SMD8 & 235 & 326 & 329 & 411 & 47 \\ \hline 
    \end{tabular}
\end{table}



\begin{table}[htbp]
    \caption{Number of lower level evaluations obtained from 31 independent runs by QBCA.}
    \label{tab:ll-evals}
    \centering
    \begin{tabular}{cccccc}
        \hline
        & Best &  Median &  Mean &  Worst &  Std. \\ \hline
        SMD1 & 268900 & 305000 & 304840 & 334100 & 14795 \\ \hline 
        SMD2 & 273440 & 308340 & 308750 & 346600 & 15158 \\ \hline 
        SMD3 & 344980 & 416220 & 414850 & 488010 & 29549 \\ \hline 
        SMD4 & 238550 & 321590 & 319820 & 434230 & 38909 \\ \hline 
        SMD5 & 203310 & 332840 & 329110 & 358470 & 26756 \\ \hline 
        SMD6 & 209510 & 228590 & 230070 & 257180 & 8781.3 \\ \hline 
        SMD7 & 224570 & 716140 & 710090 & 1092100 & 160500 \\ \hline 
        SMD8 & 440630 & 483680 & 484610 & 532170 & 23969 \\ \hline 
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \caption{Number of lower level evaluations obtained from 31 independent runs by QBCA. 5-dimensional}
    \label{tab:ll-evals-5}
    \centering
    \begin{tabular}{cccccc}
        \hline
            & Best &  Median &  Mean &  Worst &  Std. \\ \hline
        SMD1 & 44220 & 47951 & 48461 & 55347 & 2698 \\ \hline 
        SMD2 & 42881 & 47187 & 47333 & 52255 & 2607 \\ \hline 
        SMD3 & 45357 & 55797 & 56283 & 74136 & 6403 \\ \hline 
        SMD4 & 42066 & 45672 & 45579 & 49855 & 2188 \\ \hline 
        SMD5 & 44300 & 50547 & 50450 & 56417 & 2499 \\ \hline 
        SMD6 & 122540 & 142010 & 141710 & 156390 & 5736 \\ \hline 
        SMD7 & 128290 & 132460 & 134850 & 189760 & 10517 \\ \hline 
        SMD8 & 55204 & 63039 & 63093 & 69120 & 3671 \\ \hline 
    \end{tabular}
\end{table}


\subsection{Experiment 3}
As it can be seen in Table \ref{tab:ll-comparative-vals}, QBCA outperformed BLEAQ-II
in five test problems in terms of both, upper and lower level median accuracy
values, and BLEAQ-II was better in the remaining three SMD test problems. 

In terms of objective function evaluations, and based in Table \ref{tab:ul-comparative-fes},
it is interesting to note that QBCA saved, on average, 24\% of NFEs at the upper
level in six test problems. BLEAQ-II outperformed QBCA in the two remaining test
problems. Based on the same Table \ref{tab:ul-comparative-fes}, BLEAQ-II clearly
required less evaluations in the lower level with respect to the NFEs used by
QBCA (around 68\% less NFEs). 

Furthermore, the overall results suggest that Tykhonov regularization was useful
to generate competitive results for BO problems. However, in test problems SMD6
(multiple global solutions) and SMD7 (multi-modality at the upper level), this
strategy can be improved.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
    \caption{Median accuracy values by QBCA and BLEAQ-II obtained from 31 independent runs.}
    \label{tab:ll-comparative-vals}
    \centering
    \begin{tabular}{|c|c|c|c|c|}
%---------------------------------------------
\hline
& \multicolumn{2}{c|}{Upper Level} & \multicolumn{2}{c|}{Lower Level} \\ \hline
& QBCA & BLEAQ-II & QBCA & BLEAQ-II \\ \hline
%---------------------------------------------
SMD1 & \textbf{7.0279E-05}  &  9.91E-05 & \textbf{1.2053E-05} &          6.72E-05 \\ \hline
SMD2 & \textbf{6.5342E-05}  &  2.82E-04 & \textbf{1.5472E-05} &          3.84E-04 \\ \hline
SMD3 & 5.5641E-05    & \textbf{4.96E-06}&          1.3876E-05 & \textbf{6.26E-06} \\ \hline
SMD4 & \textbf{5.0910E-05}  &  1.54E-04 & \textbf{1.3845E-05} &          6.12E-04 \\ \hline
SMD5 & \textbf{6.7266E-05}  &  1.62E-04 & \textbf{1.1682E-05} &          3.08E-04 \\ \hline
SMD6 & 1.2483E+01  &  \textbf{1.46E-13} &          6.2809E-05 & \textbf{8.66E-16} \\ \hline
SMD7 & 1.0252E+00  &  \textbf{9.76E-02} &          3.7499E+02 & \textbf{1.25E+02} \\ \hline
SMD8 & \textbf{7.8491E-05}  &           7.46E-03 & \textbf{2.1320E-05} &          5.63E-03 \\ \hline

%---------------------------------------------
    \end{tabular}
\end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
    \caption{Median accuracy values by QBCA and BLEAQ-II obtained from 31 independent runs. 5-dimensional}
    \label{tab:ll-comparative-vals-5}
    \centering
    \begin{tabular}{|c|c|c|c|c|}
%---------------------------------------------
\hline
& \multicolumn{2}{c|}{Upper Level} & \multicolumn{2}{c|}{Lower Level} \\ \hline
& QBCA & BLEAQ-II & QBCA & BLEAQ-II \\ \hline
%---------------------------------------------
SMD1 & 3.7011E-05 & 2.2382E-05 & 6.5495E-06 & 1.8424E-05\\ \hline 
SMD2 & 3.8328E-05 & 3.3310E-05 & 5.9320E-06 & 4.6570E-05\\ \hline 
SMD3 & 2.6711E-05 & 2.7792E-05 & 4.7070E-06 & 2.1052E-05\\ \hline 
SMD4 & 3.7548E-05 & 3.6568E-06 & 2.3948E-06 & 3.6665E-05\\ \hline 
SMD5 & 3.7063E-05 & 2.8510E-05 & 8.2375E-06 & 3.9704E-05\\ \hline 
SMD6 & 1.0716E+01 & 3.1173E-14 & 4.1155E-05 & 2.3946E-16\\ \hline 
SMD7 & 7.7884E-01 & 2.8465E-05 & 1.2500E+02 & 3.9252E-05\\ \hline 
SMD8 & 5.8509E-05 & 2.7537E-05 & 7.5347E-06 & 2.1607E-05\\ \hline 
%---------------------------------------------
    \end{tabular}
\end{table}

% -------------------------------------------------


\begin{table}[t]
    \caption{Median NFEs values by QBCA and BLEAQ-II obtained from 31 independent runs.}
    \label{tab:ul-comparative-fes}
    \centering
    \begin{tabular}{|c|c|c|c|c|}
%---------------------------------------------
\hline
& \multicolumn{2}{c|}{Upper Level} & \multicolumn{2}{c|}{Lower Level} \\ \hline
& QBCA & BLEAQ-II & QBCA & BLEAQ-II \\ \hline
%---------------------------------------------
SMD1 & \textbf{1553}  & 1600          &  305000 & \textbf{116088} \\ \hline
SMD2 & \textbf{1525}  & 1925          &  308340 & \textbf{113504} \\ \hline
SMD3 & \textbf{1580}  & 1630          &  416220 & \textbf{122542} \\ \hline
SMD4 & \textbf{1401} & 1750  &  321590 & \textbf{70906} \\ \hline
SMD5 & \textbf{1615}  & 3031          &  332840 & \textbf{147289} \\ \hline
SMD6 & 1981 &  \textbf{1016} &   228590& \textbf{7055} \\ \hline
SMD7 & 5011 &  \textbf{2104} & 716140  & \textbf{130195} \\ \hline
SMD8 & \textbf{2432}  & 5569          &  483680 & \textbf{289886} \\ \hline
%---------------------------------------------
    \end{tabular}
\end{table}

\begin{table}[t]
    \caption{Median NFEs values by QBCA and BLEAQ-II obtained from 31 independent runs. 5-dimensional}
    \label{tab:ul-comparative-fes-5}
    \centering
    \begin{tabular}{|c|c|c|c|c|}
%---------------------------------------------
\hline
& \multicolumn{2}{c|}{Upper Level} & \multicolumn{2}{c|}{Lower Level} \\ \hline
& QBCA & BLEAQ-II & QBCA & BLEAQ-II \\ \hline
%---------------------------------------------
SMD1 & \textbf{210} & 236 & 47951 & \textbf{13692}  \\ \hline 
SMD2 & \textbf{192} & 320 & 47187 & \textbf{11240}  \\ \hline 
SMD3 & \textbf{217} & 346 & 55797 & \textbf{20320}  \\ \hline 
SMD4 & \textbf{167} & 426 & 45672 & \textbf{13094}  \\ \hline 
SMD5 & \textbf{207} & 494 & 50547 & \textbf{21548}  \\ \hline 
SMD6 & 2000 & \textbf{566} & 142010 & \textbf{2935}  \\ \hline 
SMD7 & 2000 & \textbf{494} & 132460 & \textbf{19236}  \\ \hline 
SMD8 & \textbf{326} & 1561 & 63039 & \textbf{58310}  \\ \hline 
%---------------------------------------------
    \end{tabular}
\end{table}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{img/SMD_5_5.pdf}
    \caption{Convergence plot for median upper level function value obtained from 31 independent runs of QBCA on SMD1-SMD5 and SMD8 problems with $D_{ul} = D_{ll} = 5$. Log scale is used for visualization purposes.}
\end{figure*}

% section numerical_result (end)





\section{Conclusions} % (fold)
\label{sec:conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%

We proposed a population-based metaheuristic for the nested case to solve bilevel
optimization problems. Tykhonov regularization was used to deal with multiple
optimal solutions at the lower level; the upper and lower level searches were
carried out by using a variation operator inspired in the center of mass, while
the lower level search was assisted by a quasi-Newton method to handle infeasible
solutions. Some assumptions were made in order to integrate this physics-inspired
metaheuristic operator with the quasi-Newton method and a problem relaxation.

The results of three experiments when solving eight test problems showed that QBCA
is robust to find competitive results based on accuracy in both, upper and lower
levels. Such robustness was also found regarding the number of evaluations required
in the upper level. However, in the lower level more variations in the number of
evaluations were observed. Finally, QBCA provided a competitive performance against
the state-of-the-art BLEAQ-II algorithm considering accuracy in both, upper and 
lower levels. Furthermore, QBCA saved evaluations in the upper level, but required
more of them in the lower level when compared against BLEAQ-II. 

Future research will focus on utilizing some theorems about Tykhonov regularization
(dynamic values for $\alpha$ and $\beta$) in order to ensure convergence, theoretically
speaking and also looking for mechanisms to decrease the number of evaluations
required in the lower level.

More information (code, tutorials, etc.) about bilevel optimization can be found
for free at our website \verb|https://bi-level.org|.
% section conclusions (end)


\section*{Acknowledgments} % (fold)
The first author acknowledges the support of the Mexican National Council of Science
and Technology (CONACyT) through a scholarship to pursue graduate studies at the
University of Veracruz. The second author acknowledges support from CONACyT through
project No. 220522. 




\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,/home/jesus/Dropbox/escuela/conf/references,/home/jesus/Dropbox/escuela/conf/references-bilevel}

\end{document}


% fn & Best &  Median &  Mean &  Worst &  Std. \\ 

% 1 & 4.5863e-16 & 2.2382e-05 & 4.0619e-05 & 1.9653e-04 & 4.5001e-05 \\ \hline 
% 2 & 2.1918e-06 & 3.3310e-05 & 5.6452e-05 & 4.1273e-04 & 7.6371e-05 \\ \hline 
% 3 & 1.1316e-09 & 2.7792e-05 & 3.1475e-05 & 8.7653e-05 & 2.9327e-05 \\ \hline 
% 4 & 3.3100e-08 & 3.6568e-06 & 3.9676e-05 & 3.4330e-04 & 8.0452e-05 \\ \hline 
% 5 & 3.1308e-06 & 2.8510e-05 & 3.6516e-05 & 1.8344e-04 & 3.5229e-05 \\ \hline 
% 6 & 1.9080e-16 & 3.1173e-14 & 4.9277e-06 & 1.5275e-04 & 2.7435e-05 \\ \hline 
% 7 & 6.2096e-07 & 2.8465e-05 & 2.5361e-02 & 9.8207e-02 & 4.3668e-02 \\ \hline 
% 8 & 2.0677e-07 & 2.7537e-05 & 1.0688e-04 & 1.0679e-03 & 2.5455e-04 \\ \hline 
% Lower level
% fn & Best &  Median &  Mean &  Worst &  Std. \\ 

% 1 & 4.5766e-16 & 1.8424e-05 & 2.8316e-05 & 1.1348e-04 & 3.1049e-05 \\ \hline 
% 2 & 4.4464e-06 & 4.6570e-05 & 9.9648e-05 & 9.3998e-04 & 1.8546e-04 \\ \hline 
% 3 & 1.0685e-09 & 2.1052e-05 & 3.1275e-05 & 9.9504e-05 & 2.9662e-05 \\ \hline 
% 4 & 1.0996e-06 & 3.6665e-05 & 8.4546e-05 & 8.4616e-04 & 1.5849e-04 \\ \hline 
% 5 & 1.9184e-06 & 3.9704e-05 & 6.6987e-05 & 4.3258e-04 & 9.0595e-05 \\ \hline 
% 6 & 2.5842e-17 & 2.3946e-16 & 3.4793e-09 & 1.0783e-07 & 1.9367e-08 \\ \hline 
% 7 & 1.3606e-06 & 3.9252e-05 & 6.3005e+01 & 2.4459e+02 & 1.0860e+02 \\ \hline 
% 8 & 1.7342e-07 & 2.1607e-05 & 4.5825e-05 & 3.2994e-04 & 7.4065e-05 \\ \hline 
% ======================== NFEs
% Upper level
% fn & Best &  Median &  Mean &  Worst &  Std. \\ 

% 1 & 1.2800e+02 & 2.3600e+02 & 3.8355e+02 & 1.1870e+03 & 3.3121e+02 \\ \hline 
% 2 & 1.1200e+02 & 3.2000e+02 & 3.5374e+02 & 1.1580e+03 & 2.4990e+02 \\ \hline 
% 3 & 1.6700e+02 & 3.4600e+02 & 4.1077e+02 & 1.2980e+03 & 2.3946e+02 \\ \hline 
% 4 & 1.4300e+02 & 4.2600e+02 & 5.5916e+02 & 1.4680e+03 & 3.6565e+02 \\ \hline 
% 5 & 1.6000e+02 & 4.9400e+02 & 6.8906e+02 & 1.8130e+03 & 4.7526e+02 \\ \hline 
% 6 & 1.7400e+02 & 5.6600e+02 & 5.6658e+02 & 1.3240e+03 & 2.6439e+02 \\ \hline 
% 7 & 1.5500e+02 & 4.9400e+02 & 5.6148e+02 & 2.5510e+03 & 4.7183e+02 \\ \hline 
% 8 & 5.3700e+02 & 1.5610e+03 & 1.7747e+03 & 4.3790e+03 & 9.9650e+02 \\ \hline 
% Lower level
% fn & Best &  Median &  Mean &  Worst &  Std. \\ 

% 1 & 1.0492e+04 & 1.3692e+04 & 1.5350e+04 & 3.2743e+04 & 5.5010e+03 \\ \hline 
% 2 & 7.9440e+03 & 1.1240e+04 & 1.3829e+04 & 2.7972e+04 & 5.5791e+03 \\ \hline 
% 3 & 1.3148e+04 & 2.0320e+04 & 2.1512e+04 & 3.3696e+04 & 5.0018e+03 \\ \hline 
% 4 & 7.6260e+03 & 1.3094e+04 & 1.4936e+04 & 3.0312e+04 & 6.0690e+03 \\ \hline 
% 5 & 1.3898e+04 & 2.1548e+04 & 2.5085e+04 & 4.5407e+04 & 9.3279e+03 \\ \hline 
% 6 & 1.0390e+03 & 2.9350e+03 & 2.8926e+03 & 7.5050e+03 & 1.4548e+03 \\ \hline 
% 7 & 1.0282e+04 & 1.9236e+04 & 2.2678e+04 & 5.2028e+04 & 1.1598e+04 \\ \hline 
% 8 & 2.3053e+04 & 5.8310e+04 & 6.0837e+04 & 1.1494e+05 & 2.4758e+04 \\ \hline 
